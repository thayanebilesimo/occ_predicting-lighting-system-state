{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from logging.handlers import TimedRotatingFileHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALGORITHM = \"knn\"\n",
    "RANDOM_SEED = 42\n",
    "TAMANHO_DA_JANELA_EM_DIAS = 7\n",
    "TIPO_ILUMINANCIA = \"caixa\"\n",
    "\n",
    "LOG_SAVE_PATH = \"logs_iluminancia\"\n",
    "LOG_FILENAME = f\"{ALGORITHM}_{TAMANHO_DA_JANELA_EM_DIAS}_dias_iluminancia_{TIPO_ILUMINANCIA}.log\"\n",
    "\n",
    "RESULTS_SAVE_PATH = \"logs_iluminancia\"\n",
    "DAILY_RESULTS_FILENAME = f\"{ALGORITHM}_{TAMANHO_DA_JANELA_EM_DIAS}_dias_iluminancia_{TIPO_ILUMINANCIA}.csv\"\n",
    "RESULTS_FILENAME = f\"{ALGORITHM}_iluminancia_{TIPO_ILUMINANCIA}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_setup(filename, log_level):\n",
    "    logger = logging.getLogger(filename)\n",
    "    logger.setLevel(log_level)\n",
    "    formatter = logging.Formatter(fmt='%(asctime)s %(name)-12s %(levelname)-8s %(message)s',\n",
    "                                  datefmt='%m-%d-%y %H:%M:%S')\n",
    "    fh = TimedRotatingFileHandler(filename, when='midnight')\n",
    "    fh.setFormatter(formatter)\n",
    "    sh = logging.StreamHandler(sys.stdout)\n",
    "    sh.setLevel(log_level)\n",
    "    sh.setFormatter(formatter)\n",
    "    logger.addHandler(fh)\n",
    "    logger.addHandler(sh)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(LOG_SAVE_PATH, exist_ok=True)\n",
    "os.makedirs(RESULTS_SAVE_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(f\"{LOG_SAVE_PATH}/{LOG_FILENAME}\"):\n",
    "    os.remove(f\"{LOG_SAVE_PATH}/{LOG_FILENAME}\")\n",
    "logger = log_setup(f\"{LOG_SAVE_PATH}/{LOG_FILENAME}\", logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"mai_2022_fev_2023.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_names = list(data.columns)[:-1]\n",
    "logger.info(f\"features_names: {features_names}\")\n",
    "logger.info(f\"len(features_names): {len(features_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_names_iluminancia_teto = features_names.copy()\n",
    "features_names_iluminancia_caixa = features_names.copy()\n",
    "\n",
    "features_names_iluminancia_teto.remove('iluminancia_caixa')\n",
    "features_names_iluminancia_caixa.remove('iluminancia_teto')\n",
    "\n",
    "logger.info(f\"features_names_iluminancia_teto: {features_names_iluminancia_teto}\")\n",
    "logger.info(f\"features_names_iluminancia_caixa: {features_names_iluminancia_caixa}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TIPO_ILUMINANCIA == \"teto\":\n",
    "    features_names = features_names_iluminancia_teto.copy()\n",
    "elif TIPO_ILUMINANCIA == \"caixa\":\n",
    "    features_names = features_names_iluminancia_caixa.copy()\n",
    "else:\n",
    "    raise ValueError(\"Escolha entre os dois tipos de iluminâncias: 'teto' ou 'caixa'\")\n",
    "\n",
    "logger.info(f\"features_names: {features_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date'] = pd.to_datetime(dict(year=data.data_ano, month=data.data_mes, day=data.data_dia))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_or_append_avg_metrics(colunas_geral, metrics_geral, path):\n",
    "    new_data = pd.DataFrame(columns=colunas_geral, data=metrics_geral)\n",
    "    \n",
    "    if os.path.isfile(path):\n",
    "        data = pd.read_csv(path)\n",
    "        data = pd.concat([data, new_data])\n",
    "        data.to_csv(path, index=False)\n",
    "        print(f\"Arquivo já existe: {path}\")\n",
    "    else:\n",
    "        print(\"Arquivo não existe\")\n",
    "        new_data.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas = [\"Teste\", \"Precision\", \"Recall\", \"F1\", \"Accuracy\"]\n",
    "\n",
    "metrics = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "f1_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "metrics_w = []\n",
    "precision_w_list = []\n",
    "recall_w_list = []\n",
    "f1_w_list = []\n",
    "accuracy_w_list = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "while True:\n",
    "    data = original_data.copy()\n",
    "    logger.info(f\"==================== Teste nº{i+1} (i={i}) ====================\")\n",
    "\n",
    "    min_date = datetime.strptime(str(data['date'].min())[:10], '%Y-%m-%d')\n",
    "    max_date = datetime.strptime(str(data['date'].max())[:10], '%Y-%m-%d')\n",
    "    \n",
    "    # Dados de treino\n",
    "    # Aqui foi colocado o \"+30\" para começar depois da data do primeiro teste\n",
    "    initial_date_train = min_date + timedelta(days = i) \n",
    "    final_date_train = initial_date_train + timedelta(days = TAMANHO_DA_JANELA_EM_DIAS)\n",
    "    if final_date_train > max_date:\n",
    "        logger.info(\"A data final de treino é maior que a data máxima\")\n",
    "        break\n",
    "\n",
    "    # Dados de teste\n",
    "    initial_date_test = final_date_train\n",
    "    final_date_test = initial_date_test + timedelta(days = TAMANHO_DA_JANELA_EM_DIAS)\n",
    "\n",
    "    if initial_date_test > max_date:\n",
    "        logger.info(\"A data inicial de teste é maior que a data máxima\")\n",
    "        break\n",
    "    elif final_date_test > max_date:\n",
    "        logger.info(\"A data final de teste é maior que a data máxima\")\n",
    "        break\n",
    "    \n",
    "    logger.info(f\"initial_date_train: {initial_date_train}\")\n",
    "    logger.info(f\"final_date_train: {final_date_train}\")\n",
    "    logger.info(f\"initial_date_test: {initial_date_test}\")\n",
    "    logger.info(f\"final_date_test: {final_date_test}\")\n",
    "\n",
    "    data_train = data.loc[(data['date'] >= initial_date_train) & (data['date'] < final_date_train)]\n",
    "    data_test = data.loc[(data['date'] >= initial_date_test) & (data['date'] < final_date_test)]\n",
    "\n",
    "    logger.info(f\"min_date_train: {data_train['date'].min()}\")\n",
    "    logger.info(f\"max_date_train: {data_train['date'].max()}\")\n",
    "    logger.info(f\"min_date_test: {data_test['date'].min()}\")\n",
    "    logger.info(f\"max_date_test: {data_test['date'].max()}\")\n",
    "    \n",
    "    if str(data_train['date'].min()) == \"NaT\" or \\\n",
    "        str(data_train['date'].max()) == \"NaT\" or \\\n",
    "        str(data_test['date'].min()) == \"NaT\" or \\\n",
    "        str(data_test['date'].max()) == \"NaT\":\n",
    "        logger.info(\"Erro: amostras vazias.\")\n",
    "        i += 1\n",
    "        continue\n",
    "\n",
    "    logger.info(f\"Distribuição de classes - Treino: {data_train['output'].value_counts()}\")\n",
    "    logger.info(f\"Distribuição de classes - Teste: {data_test['output'].value_counts()}\")\n",
    "\n",
    "    X_train = data_train[features_names]\n",
    "\n",
    "    X_train = X_train.values.tolist()\n",
    "    y_train = data_train['output'].values.tolist()\n",
    "\n",
    "    X_test = data_test[features_names]\n",
    "    if len(X_train) < 5 or len(X_test) < 5:\n",
    "        logger.info(\"Quantidade de amostras é menor que a quantidade de vizinhos...\")\n",
    "        i += 1\n",
    "        continue\n",
    "\n",
    "    X_test = X_test.values.tolist()\n",
    "    y_test = data_test['output'].values.tolist()\n",
    "\n",
    "    # Train-Test\n",
    "    logger.info(\"-- Train/Test --\")\n",
    "    clf = KNeighborsClassifier()\n",
    "    try:\n",
    "        clf.fit(X_train, y_train)\n",
    "    except Exception as error:\n",
    "        logger.info(f\"Erro: {error}\")\n",
    "        continue\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    metrics.append([i+1, precision, recall, f1, acc])\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    f1_list.append(f1)\n",
    "    accuracy_list.append(acc)\n",
    "\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas_geral = [\n",
    "    \"Janela\",\n",
    "    \"Media_Precision\",\n",
    "    \"Media_Recall\",\n",
    "    \"Media_F1\",\n",
    "    \"Media_Accuracy\",\n",
    "    \"Desvio_Precision\",\n",
    "    \"Desvio_Recall\",\n",
    "    \"Desvio_F1\",\n",
    "    \"Desvio_Accuracy\"\n",
    "]\n",
    "metrics_geral = [\n",
    "    [TAMANHO_DA_JANELA_EM_DIAS,\n",
    "    np.mean(precision_list),\n",
    "    np.mean(recall_list),\n",
    "    np.mean(f1_list),\n",
    "    np.mean(accuracy_list),\n",
    "    np.std(precision_list),\n",
    "    np.std(recall_list),\n",
    "    np.std(f1_list),\n",
    "    np.std(accuracy_list)]\n",
    "]\n",
    "\n",
    "save_or_append_avg_metrics(colunas_geral, metrics_geral, f\"{RESULTS_SAVE_PATH}/{RESULTS_FILENAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultados diários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analise_dias = pd.DataFrame(columns=colunas, data=metrics)\n",
    "analise_dias.to_csv(f\"{RESULTS_SAVE_PATH}/{DAILY_RESULTS_FILENAME}\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr_prime",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
